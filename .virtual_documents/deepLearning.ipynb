


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dla-1-2/m21/lms/starter/charity_data.csv")
application_df.head()


# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_wanted_cols_df = application_df.drop(columns=["EIN","NAME"])
application_wanted_cols_df.head()


# Determine the number of unique values in each column.
application_cat = application_wanted_cols_df.dtypes[application_wanted_cols_df.dtypes == "object"].index.tolist()
application_wanted_cols_df[application_cat].nunique()


# Look at APPLICATION_TYPE value counts to identify and replace with "Other"
app_type_counts = application_wanted_cols_df["APPLICATION_TYPE"].value_counts()
app_type_counts


# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
cutoff_val = 200
application_types_to_replace = app_type_counts[app_type_counts < cutoff_val]
application_types_to_replace = application_types_to_replace.index.values.tolist()

# Replace in dataframe
for app in application_types_to_replace:
    application_wanted_cols_df['APPLICATION_TYPE'] = application_wanted_cols_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_wanted_cols_df['APPLICATION_TYPE'].value_counts()


# Look at CLASSIFICATION value counts to identify and replace with "Other"
classif_counts = application_wanted_cols_df["CLASSIFICATION"].value_counts()
classif_counts


# You may find it helpful to look at CLASSIFICATION value counts > 10 :)
cutoff_val = 10
classifications_to_see = classif_counts[classif_counts > cutoff_val]
classifications_to_see = classifications_to_see
classifications_to_see


# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
cutoff_val = 1000
classifications_to_replace = classif_counts[classif_counts < cutoff_val]
classifications_to_replace = classifications_to_replace.index.values.tolist()
classifications_to_replace

# Replace in dataframe
for cls in classifications_to_replace:
    application_wanted_cols_df['CLASSIFICATION'] = application_wanted_cols_df['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure replacement was successful
application_wanted_cols_df['CLASSIFICATION'].value_counts()


# Convert categorical data to numeric with `pd.get_dummies`
application_dummies_df = pd.get_dummies(application_wanted_cols_df)
application_dummies_df.head()


# Split our preprocessed data into our features and target arrays
y = application_dummies_df.IS_SUCCESSFUL.values
X = application_dummies_df.drop(columns="IS_SUCCESSFUL").values

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)





# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
# try 2 hidden layers
nn = tf.keras.models.Sequential()


# First hidden layer
# 40 * 2.5 = 110 neurons (approximating as 40)
nn.add(tf.keras.layers.Dense(units=100, activation="relu", input_dim=43))

# Second hidden layer
# 100 / 2 = 50
nn.add(tf.keras.layers.Dense(units=50, activation="relu"))

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()


# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


# Train the model
fit_model = nn.fit(X_train_scaled,y_train,epochs=100)


# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
nn.save('AlphabetSoupCharity.h5')





# Create a method that creates a new Sequential model with hyperparameter options
def create_model(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function to use in hidden layers
    activation = hp.Choice('activation',['relu','tanh','sigmoid'])
    
    # Allow kerastuner to decide number of neurons in first layer
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=30,
        max_value=150,
        step=20), activation=activation, input_dim=43))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(hp.Int('num_layers', 1, 6)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=30,
            max_value=150,
            step=20),
            activation=activation))
    
    nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # Compile the model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])
    
    return nn_model


# Import the kerastuner library
import keras_tuner as kt

tuner = kt.Hyperband(
    create_model,
    objective="val_accuracy",
    max_epochs=20,
    hyperband_iterations=2)


# Run the kerastuner search for best hyperparameters
tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))


import pprint 
pp = pprint.PrettyPrinter(indent=4)

# Get top 3 model hyperparameters and print the values
top_hyper = tuner.get_best_hyperparameters(3)
for param in top_hyper:
    pp.pprint(param.values)





# Try again with higher epoch, also try higher number of min neurons, as min best neurons was 70
# Try with only "relu" activation
# Create a method that creates a new Sequential model with hyperparameter options
def create_model_2(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function to use in hidden layers
    activation = hp.Choice('activation',['relu'])
    
    # Allow kerastuner to decide number of neurons in first layer
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=70,
        max_value=170,
        step=20), activation=activation, input_dim=43))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(hp.Int('num_layers', 1, 6)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=70,
            max_value=170,
            step=20),
            activation=activation))
    
    nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # Compile the model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])
    
    return nn_model


# make tuner
tuner = kt.Hyperband(
    create_model_2,
    objective="val_accuracy",
    max_epochs=70,
    hyperband_iterations=2)


# Run the kerastuner search for best hyperparameters
tuner.search(X_train_scaled,y_train,epochs=70,validation_data=(X_test_scaled,y_test))





# Get top 3 model hyperparameters and print the values
top_hyper = tuner.get_best_hyperparameters(3)
for param in top_hyper:
    pp.pprint(param.values)





# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_wanted_cols_df = application_df.drop(columns=["EIN","NAME"])
application_wanted_cols_df.head()


# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
cutoff_val = 20
application_types_to_replace = app_type_counts[app_type_counts < cutoff_val]
application_types_to_replace = application_types_to_replace.index.values.tolist()

# Replace in dataframe
for app in application_types_to_replace:
    application_wanted_cols_df['APPLICATION_TYPE'] = application_wanted_cols_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_wanted_cols_df['APPLICATION_TYPE'].value_counts()


# Choose a small cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
cutoff_val = 10
classifications_to_replace = classif_counts[classif_counts < cutoff_val]
classifications_to_replace = classifications_to_replace.index.values.tolist()
classifications_to_replace

# Replace in dataframe
for cls in classifications_to_replace:
    application_wanted_cols_df['CLASSIFICATION'] = application_wanted_cols_df['CLASSIFICATION'].replace(cls,"Other_Small")

# Check to make sure replacement was successful
application_wanted_cols_df['CLASSIFICATION'].value_counts()


# Choose a medium cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
cutoff_val = 96
classifications_to_replace = classif_counts[classif_counts < cutoff_val]
classifications_to_replace = classifications_to_replace.index.values.tolist()
classifications_to_replace

# Replace in dataframe
for cls in classifications_to_replace:
    application_wanted_cols_df['CLASSIFICATION'] = application_wanted_cols_df['CLASSIFICATION'].replace(cls,"Other_Med")

# Check to make sure replacement was successful
application_wanted_cols_df['CLASSIFICATION'].value_counts()


# Convert categorical data to numeric with `pd.get_dummies`
application_dummies_df = pd.get_dummies(application_wanted_cols_df)
application_dummies_df.head()


# Split our preprocessed data into our features and target arrays
y = application_dummies_df.IS_SUCCESSFUL.values
X = application_dummies_df.drop(columns="IS_SUCCESSFUL").values

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)


# Create a method that creates a new Sequential model with hyperparameter options
def create_model_3(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function to use in hidden layers
    activation = hp.Choice('activation',['relu','tanh','sigmoid'])
    
    # Allow kerastuner to decide number of neurons in first layer
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=70,
        max_value=170,
        step=20), activation=activation, input_dim=53))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(hp.Int('num_layers', 1, 6)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=70,
            max_value=170,
            step=20),
            activation=activation))
    
    nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # Compile the model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])
    
    return nn_model


# Tuner
tuner = kt.Hyperband(
    create_model_3,
    objective="val_accuracy",
    max_epochs=40,
    hyperband_iterations=2)


# Run the kerastuner search for best hyperparameters
tuner.search(X_train_scaled,y_train,epochs=40,validation_data=(X_test_scaled,y_test))





# Get top 3 model hyperparameters and print the values
top_hyper = tuner.get_best_hyperparameters(3)
for param in top_hyper:
    pp.pprint(param.values)





# Evaluate best model against full test data
best_model = tuner.get_best_models(1)[0]
model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")



# Export our model to HDF5 file
nn.save('AlphabetSoupCharity_Optimisation.h5')




